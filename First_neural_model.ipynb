{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-15 23:45:04.654494: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-15 23:45:04.687986: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-15 23:45:04.688537: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-15 23:45:05.215962: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" #model will be trained on GPU 1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.misc import derivative\n",
    "import scipy.integrate as intg\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import Constant, RandomNormal\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from astroNN.nn.layers import MCDropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zCMB</th>\n",
       "      <th>zHEL</th>\n",
       "      <th>MU</th>\n",
       "      <th>MUERR</th>\n",
       "      <th>MUERR_SYS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.04131</td>\n",
       "      <td>0.04131</td>\n",
       "      <td>36.31666</td>\n",
       "      <td>0.00969</td>\n",
       "      <td>0.00674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.06876</td>\n",
       "      <td>0.06876</td>\n",
       "      <td>37.44798</td>\n",
       "      <td>0.00541</td>\n",
       "      <td>0.00627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.13121</td>\n",
       "      <td>0.13121</td>\n",
       "      <td>38.94492</td>\n",
       "      <td>0.01969</td>\n",
       "      <td>0.00868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.18499</td>\n",
       "      <td>0.18499</td>\n",
       "      <td>39.75545</td>\n",
       "      <td>0.01338</td>\n",
       "      <td>0.00658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.24503</td>\n",
       "      <td>0.24503</td>\n",
       "      <td>40.42469</td>\n",
       "      <td>0.01068</td>\n",
       "      <td>0.00590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      zCMB     zHEL        MU    MUERR  MUERR_SYS\n",
       "0  0.04131  0.04131  36.31666  0.00969    0.00674\n",
       "1  0.06876  0.06876  37.44798  0.00541    0.00627\n",
       "2  0.13121  0.13121  38.94492  0.01969    0.00868\n",
       "3  0.18499  0.18499  39.75545  0.01338    0.00658\n",
       "4  0.24503  0.24503  40.42469  0.01068    0.00590"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder = 'Data_SNIa_LSST/'\n",
    "dfp = pd.read_csv(data_folder+'hubble_diagram_Pr.txt', skiprows=4, sep=' ')\n",
    "dfs = pd.read_csv(data_folder+'hubble_diagram_Sr.txt', skiprows=4, sep=' ')\n",
    "Np = len(dfp.values)\n",
    "Ns= len(dfs.values)\n",
    "dfp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04131, 0.06876, 0.13121, 0.18499, 0.24503, 0.31077, 0.38392,\n",
       "       0.46752, 0.55669, 0.66538, 0.78147, 0.92328, 1.08255, 1.24447])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfp['zCMB'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fddd586d510>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(dfp['zCMB'].values, dfp['MU'].values, xerr=dfp['MUERR_SYS'].values+dfp['MUERR_SYS'].values, fmt='.', color='b', elinewidth=8, ecolor='red', label='photometric')\n",
    "plt.errorbar(dfs['zCMB'].values, dfs['MU'].values, xerr=dfs['MUERR_SYS'].values+dfs['MUERR_SYS'].values, fmt='*', color='g', elinewidth=8, ecolor='yellow', label='spectroscopic')\n",
    "plt.ylabel(\"$\\mu$\")\n",
    "plt.xlabel(\"Redshift z\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zCMB</th>\n",
       "      <th>zHEL</th>\n",
       "      <th>MU</th>\n",
       "      <th>MUERR</th>\n",
       "      <th>MUERR_SYS</th>\n",
       "      <th>errors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.04131</td>\n",
       "      <td>0.04131</td>\n",
       "      <td>36.31666</td>\n",
       "      <td>0.00969</td>\n",
       "      <td>0.00674</td>\n",
       "      <td>0.01348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.06876</td>\n",
       "      <td>0.06876</td>\n",
       "      <td>37.44798</td>\n",
       "      <td>0.00541</td>\n",
       "      <td>0.00627</td>\n",
       "      <td>0.01254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.13121</td>\n",
       "      <td>0.13121</td>\n",
       "      <td>38.94492</td>\n",
       "      <td>0.01969</td>\n",
       "      <td>0.00868</td>\n",
       "      <td>0.01736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.18499</td>\n",
       "      <td>0.18499</td>\n",
       "      <td>39.75545</td>\n",
       "      <td>0.01338</td>\n",
       "      <td>0.00658</td>\n",
       "      <td>0.01316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.24503</td>\n",
       "      <td>0.24503</td>\n",
       "      <td>40.42469</td>\n",
       "      <td>0.01068</td>\n",
       "      <td>0.00590</td>\n",
       "      <td>0.01180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      zCMB     zHEL        MU    MUERR  MUERR_SYS   errors\n",
       "0  0.04131  0.04131  36.31666  0.00969    0.00674  0.01348\n",
       "1  0.06876  0.06876  37.44798  0.00541    0.00627  0.01254\n",
       "2  0.13121  0.13121  38.94492  0.01969    0.00868  0.01736\n",
       "3  0.18499  0.18499  39.75545  0.01338    0.00658  0.01316\n",
       "4  0.24503  0.24503  40.42469  0.01068    0.00590  0.01180"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfp['errors'] = dfp['MUERR_SYS'].values+dfp['MUERR_SYS'].values\n",
    "dfp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = dfp[['zCMB', 'MU', 'errors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zCMB</th>\n",
       "      <th>MU</th>\n",
       "      <th>errors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.04131</td>\n",
       "      <td>36.31666</td>\n",
       "      <td>0.01348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.06876</td>\n",
       "      <td>37.44798</td>\n",
       "      <td>0.01254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.13121</td>\n",
       "      <td>38.94492</td>\n",
       "      <td>0.01736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.18499</td>\n",
       "      <td>39.75545</td>\n",
       "      <td>0.01316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.24503</td>\n",
       "      <td>40.42469</td>\n",
       "      <td>0.01180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      zCMB        MU   errors\n",
       "0  0.04131  36.31666  0.01348\n",
       "1  0.06876  37.44798  0.01254\n",
       "2  0.13121  38.94492  0.01736\n",
       "3  0.18499  39.75545  0.01316\n",
       "4  0.24503  40.42469  0.01180"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomize = np.random.permutation(Np)\n",
    "data_p = dfp.values[randomize]\n",
    "\n",
    "z = data_p[:,0]\n",
    "y = data_p[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerz = StandardScaler()\n",
    "scalerz.fit(z.reshape(-1,1))\n",
    "# apply transform\n",
    "z = scalerz.transform(z.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.8\n",
    "ntrain = int(split * len(z))\n",
    "indx = [ntrain]\n",
    "z_train, z_test = np.split(z, indx)\n",
    "y_train, y_test = np.split(y, indx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RHSquared_a_owacdm(a, w0, wa, Om):\n",
    "    rhow = a**(-3*(1.0+w0+wa))*np.exp(-3*wa*(1-a))\n",
    "    return (Om/a**3+(1.0-Om)*rhow)\n",
    "\n",
    "def DistIntegrand_a(a, w0, wa, Om):\n",
    "        return 1./np.sqrt(RHSquared_a_owacdm(a, w0, wa, Om))/a**2\n",
    "    \n",
    "def Da_z(z, w0, wa, Om):\n",
    "        r = intg.quad(DistIntegrand_a, 1./(1+z), 1, args=(w0, wa, Om))\n",
    "        r = r[0]\n",
    "        return r\n",
    "\n",
    "### Hay que revisar esta constante en el return\n",
    "def distance_modulus(z, w0=-1, wa=0.0, Om=0.23):\n",
    "    return 5*np.log10(Da_z(z, w0, wa, Om)*(1+z))+24\n",
    "# +43\n",
    "\n",
    "zmodel = np.linspace(0.01, 2.4, 100)\n",
    "flcdm = []\n",
    "# fowacdm1 = []\n",
    "# fowacdm2 = []\n",
    "# fowacdm3 = []\n",
    "\n",
    "om = 0.27\n",
    "for zzz in zmodel:\n",
    "    flcdm.append(distance_modulus(zzz, w0=-1, wa=0, Om=om))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11, 1), (11, 1), (3, 1), (3, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy = y[:,0].reshape(-1,1)\n",
    "zz = z\n",
    "split = 0.8\n",
    "ntrain = int(split * len(zz))\n",
    "indx = [ntrain]\n",
    "zz_train, zz_test = np.split(zz, indx)\n",
    "yy_train, yy_test = np.split(yy, indx)\n",
    "np.shape(zz_train), np.shape(yy_train), np.shape(zz_test), np.shape(yy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-15 23:45:06.204251: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-06-15 23:45:06.204285: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: isidro-PC\n",
      "2023-06-15 23:45:06.204293: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: isidro-PC\n",
      "2023-06-15 23:45:06.204408: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 525.105.17\n",
      "2023-06-15 23:45:06.204432: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 525.105.17\n",
      "2023-06-15 23:45:06.204438: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 525.105.17\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               200       \n",
      "                                                                 \n",
      " MCDropout_1 (MCDropout)     (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 200)               20200     \n",
      "                                                                 \n",
      " MCDropout_2 (MCDropout)     (None, 200)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 150)               30150     \n",
      "                                                                 \n",
      " MCDropout_3 (MCDropout)     (None, 150)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 302       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,852\n",
      "Trainable params: 50,852\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def model_regression_dropout(num_hidden):\n",
    "    # Defeine Keras model for regression\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(batch_input_shape=((None, 1))))\n",
    "    model.add(Dense(units=num_hidden[0], \n",
    "#                     kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "#                     kernel_initializer='he_normal', \n",
    "                    activation='relu'))\n",
    "    model.add(MCDropout(0.3))\n",
    "    model.add(Dense(units=num_hidden[1], \n",
    "#                     kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "#                     kernel_initializer='he_normal', \n",
    "                    activation='relu'))\n",
    "    model.add(MCDropout(0.3))\n",
    "    model.add(Dense(units=num_hidden[2], \n",
    "#                     kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "#                     kernel_initializer='he_normal', \n",
    "                    activation='relu'))\n",
    "    model.add(MCDropout(0.3))\n",
    "    model.add(Dense(units=2, activation=\"linear\"))\n",
    "    return model\n",
    "\n",
    "#Define some parameter\n",
    "batch_size = 2\n",
    "# optimizer = Adam(lr=.005)\n",
    "optimizer = Adam(lr=0.0001)\n",
    "\n",
    "\n",
    "# Compile Keras model\n",
    "model = model_regression_dropout(num_hidden=[100, 200, 150])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "6/6 [==============================] - 1s 22ms/step - loss: 851.2102 - val_loss: 867.0630\n",
      "Epoch 2/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 839.2486 - val_loss: 835.8746\n",
      "Epoch 3/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 826.7535 - val_loss: 814.9843\n",
      "Epoch 4/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 806.2872 - val_loss: 775.7064\n",
      "Epoch 5/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 767.6808 - val_loss: 708.9456\n",
      "Epoch 6/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 731.3445 - val_loss: 611.8348\n",
      "Epoch 7/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 661.2460 - val_loss: 473.5267\n",
      "Epoch 8/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 553.8816 - val_loss: 304.3357\n",
      "Epoch 9/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 421.9336 - val_loss: 84.8847\n",
      "Epoch 10/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 281.8381 - val_loss: 73.8068\n",
      "Epoch 11/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 143.1378 - val_loss: 22.9149\n",
      "Epoch 12/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 105.6997 - val_loss: 226.5955\n",
      "Epoch 13/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 141.5167 - val_loss: 704.3874\n",
      "Epoch 14/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 122.8687 - val_loss: 123.1891\n",
      "Epoch 15/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 41.9617 - val_loss: 329.8120\n",
      "Epoch 16/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 50.4505 - val_loss: 148.7468\n",
      "Epoch 17/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 56.4734 - val_loss: 426.9440\n",
      "Epoch 18/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 51.7201 - val_loss: 372.3328\n",
      "Epoch 19/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 46.8349 - val_loss: 215.5415\n",
      "Epoch 20/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 52.4606 - val_loss: 740.4439\n",
      "Epoch 21/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 39.9326 - val_loss: 437.2244\n",
      "Epoch 22/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4588 - val_loss: 334.1258\n",
      "Epoch 23/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.7997 - val_loss: 762.2424\n",
      "Epoch 24/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 33.3065 - val_loss: 378.0513\n",
      "Epoch 25/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.9104 - val_loss: 332.1245\n",
      "Epoch 26/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 43.6267 - val_loss: 345.7957\n",
      "Epoch 27/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.6315 - val_loss: 126.3860\n",
      "Epoch 28/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.8351 - val_loss: 163.1280\n",
      "Epoch 29/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.6021 - val_loss: 224.8578\n",
      "Epoch 30/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 35.6277 - val_loss: 220.0183\n",
      "Epoch 31/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.4515 - val_loss: 151.4120\n",
      "Epoch 32/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 47.0297 - val_loss: 182.9487\n",
      "Epoch 33/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.9788 - val_loss: 272.8059\n",
      "Epoch 34/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 45.6252 - val_loss: 24.3027\n",
      "Epoch 35/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 29.6318 - val_loss: 64.3938\n",
      "Epoch 36/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.0432 - val_loss: 55.8966\n",
      "Epoch 37/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.5230 - val_loss: 231.2693\n",
      "Epoch 38/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 35.8451 - val_loss: 153.5663\n",
      "Epoch 39/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.4147 - val_loss: 85.6986\n",
      "Epoch 40/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6940 - val_loss: 48.7915\n",
      "Epoch 41/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.4070 - val_loss: 292.8388\n",
      "Epoch 42/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2986 - val_loss: 131.7344\n",
      "Epoch 43/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 41.3403 - val_loss: 144.9255\n",
      "Epoch 44/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.0335 - val_loss: 198.5089\n",
      "Epoch 45/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.6974 - val_loss: 51.4895\n",
      "Epoch 46/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.4208 - val_loss: 104.6612\n",
      "Epoch 47/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 39.2567 - val_loss: 28.7610\n",
      "Epoch 48/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 32.2964 - val_loss: 15.8220\n",
      "Epoch 49/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.0722 - val_loss: 69.7285\n",
      "Epoch 50/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.1065 - val_loss: 267.3471\n",
      "Epoch 51/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 46.2426 - val_loss: 45.7936\n",
      "Epoch 52/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.1795 - val_loss: 39.0417\n",
      "Epoch 53/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.3639 - val_loss: 51.3114\n",
      "Epoch 54/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1546 - val_loss: 54.4400\n",
      "Epoch 55/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4853 - val_loss: 75.3954\n",
      "Epoch 56/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.4659 - val_loss: 122.3116\n",
      "Epoch 57/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6701 - val_loss: 132.8811\n",
      "Epoch 58/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.4317 - val_loss: 29.3183\n",
      "Epoch 59/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7490 - val_loss: 60.9723\n",
      "Epoch 60/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8229 - val_loss: 148.1145\n",
      "Epoch 61/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.9279 - val_loss: 86.1135\n",
      "Epoch 62/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2330 - val_loss: 65.5990\n",
      "Epoch 63/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5766 - val_loss: 68.1514\n",
      "Epoch 64/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.4374 - val_loss: 74.6458\n",
      "Epoch 65/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8558 - val_loss: 26.7865\n",
      "Epoch 66/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1593 - val_loss: 166.9366\n",
      "Epoch 67/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.5267 - val_loss: 176.3140\n",
      "Epoch 68/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.9877 - val_loss: 75.0589\n",
      "Epoch 69/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9848 - val_loss: 174.9133\n",
      "Epoch 70/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.0110 - val_loss: 180.7112\n",
      "Epoch 71/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6976 - val_loss: 157.1065\n",
      "Epoch 72/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7839 - val_loss: 25.1064\n",
      "Epoch 73/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.1034 - val_loss: 73.1077\n",
      "Epoch 74/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.0392 - val_loss: 11.4699\n",
      "Epoch 75/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.2256 - val_loss: 11.5720\n",
      "Epoch 76/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0199 - val_loss: 12.7105\n",
      "Epoch 77/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.1024 - val_loss: 159.0573\n",
      "Epoch 78/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.5276 - val_loss: 125.5556\n",
      "Epoch 79/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.4443 - val_loss: 103.5729\n",
      "Epoch 80/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9247 - val_loss: 91.8677\n",
      "Epoch 81/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 28.0122 - val_loss: 176.3388\n",
      "Epoch 82/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.8158 - val_loss: 117.2949\n",
      "Epoch 83/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.6001 - val_loss: 33.8247\n",
      "Epoch 84/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7375 - val_loss: 18.9794\n",
      "Epoch 85/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.9427 - val_loss: 34.5151\n",
      "Epoch 86/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7115 - val_loss: 91.8828\n",
      "Epoch 87/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6804 - val_loss: 166.7243\n",
      "Epoch 88/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.5023 - val_loss: 65.4718\n",
      "Epoch 89/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4892 - val_loss: 88.9660\n",
      "Epoch 90/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.3055 - val_loss: 98.0731\n",
      "Epoch 91/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.6373 - val_loss: 11.1925\n",
      "Epoch 92/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.4345 - val_loss: 79.0107\n",
      "Epoch 93/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7461 - val_loss: 63.7563\n",
      "Epoch 94/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.0946 - val_loss: 46.1151\n",
      "Epoch 95/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.0708 - val_loss: 352.6764\n",
      "Epoch 96/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.3218 - val_loss: 64.1512\n",
      "Epoch 97/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.7784 - val_loss: 85.6934\n",
      "Epoch 98/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.5408 - val_loss: 33.4937\n",
      "Epoch 99/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.2530 - val_loss: 91.6925\n",
      "Epoch 100/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.3834 - val_loss: 12.7701\n",
      "Epoch 101/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.8953 - val_loss: 69.0668\n",
      "Epoch 102/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1110 - val_loss: 22.8874\n",
      "Epoch 103/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.1825 - val_loss: 16.6400\n",
      "Epoch 104/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3880 - val_loss: 185.3854\n",
      "Epoch 105/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.7014 - val_loss: 146.2869\n",
      "Epoch 106/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8009 - val_loss: 5.9610\n",
      "Epoch 107/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9342 - val_loss: 23.2933\n",
      "Epoch 108/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.7284 - val_loss: 117.2670\n",
      "Epoch 109/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.3392 - val_loss: 46.3757\n",
      "Epoch 110/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0267 - val_loss: 59.6775\n",
      "Epoch 111/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 27.2728 - val_loss: 32.6763\n",
      "Epoch 112/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.1163 - val_loss: 40.9118\n",
      "Epoch 113/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.3708 - val_loss: 81.7865\n",
      "Epoch 114/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5082 - val_loss: 25.3899\n",
      "Epoch 115/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.0908 - val_loss: 80.2002\n",
      "Epoch 116/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5947 - val_loss: 14.4768\n",
      "Epoch 117/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8697 - val_loss: 57.5214\n",
      "Epoch 118/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 38.0365 - val_loss: 6.7906\n",
      "Epoch 119/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.6130 - val_loss: 35.1482\n",
      "Epoch 120/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.6187 - val_loss: 21.7743\n",
      "Epoch 121/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.4862 - val_loss: 2.0215\n",
      "Epoch 122/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.4915 - val_loss: 31.7629\n",
      "Epoch 123/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9195 - val_loss: 37.5007\n",
      "Epoch 124/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8321 - val_loss: 90.4711\n",
      "Epoch 125/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9427 - val_loss: 60.3707\n",
      "Epoch 126/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.9178 - val_loss: 59.3918\n",
      "Epoch 127/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.9979 - val_loss: 127.8928\n",
      "Epoch 128/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.5174 - val_loss: 36.5469\n",
      "Epoch 129/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3190 - val_loss: 34.4669\n",
      "Epoch 130/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8208 - val_loss: 97.0584\n",
      "Epoch 131/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.4877 - val_loss: 75.3106\n",
      "Epoch 132/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1170 - val_loss: 86.6871\n",
      "Epoch 133/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.8146 - val_loss: 110.3909\n",
      "Epoch 134/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9024 - val_loss: 33.5299\n",
      "Epoch 135/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.5004 - val_loss: 27.3076\n",
      "Epoch 136/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.1381 - val_loss: 17.3642\n",
      "Epoch 137/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 31.0616 - val_loss: 3.5486\n",
      "Epoch 138/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.8513 - val_loss: 17.8442\n",
      "Epoch 139/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5088 - val_loss: 61.1164\n",
      "Epoch 140/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.0550 - val_loss: 43.3913\n",
      "Epoch 141/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.7476 - val_loss: 113.3225\n",
      "Epoch 142/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.2670 - val_loss: 26.6635\n",
      "Epoch 143/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.7965 - val_loss: 102.9229\n",
      "Epoch 144/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9988 - val_loss: 27.8603\n",
      "Epoch 145/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.9895 - val_loss: 82.4577\n",
      "Epoch 146/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0262 - val_loss: 29.6954\n",
      "Epoch 147/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0708 - val_loss: 23.4198\n",
      "Epoch 148/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.3228 - val_loss: 13.5565\n",
      "Epoch 149/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1337 - val_loss: 75.1745\n",
      "Epoch 150/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.9528 - val_loss: 38.4709\n",
      "Epoch 151/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.0629 - val_loss: 36.9090\n",
      "Epoch 152/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.9238 - val_loss: 17.5053\n",
      "Epoch 153/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.3895 - val_loss: 33.4673\n",
      "Epoch 154/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6110 - val_loss: 44.6306\n",
      "Epoch 155/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.0510 - val_loss: 47.9479\n",
      "Epoch 156/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2158 - val_loss: 32.8084\n",
      "Epoch 157/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6333 - val_loss: 18.4323\n",
      "Epoch 158/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8100 - val_loss: 111.6854\n",
      "Epoch 159/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.2082 - val_loss: 72.9411\n",
      "Epoch 160/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.1900 - val_loss: 20.0705\n",
      "Epoch 161/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.2533 - val_loss: 21.3207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2577 - val_loss: 24.7244\n",
      "Epoch 163/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.4680 - val_loss: 55.7423\n",
      "Epoch 164/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.4348 - val_loss: 78.3558\n",
      "Epoch 165/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.8119 - val_loss: 4.2701\n",
      "Epoch 166/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8574 - val_loss: 9.5179\n",
      "Epoch 167/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5945 - val_loss: 31.1603\n",
      "Epoch 168/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.2414 - val_loss: 39.6128\n",
      "Epoch 169/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.4063 - val_loss: 15.2784\n",
      "Epoch 170/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.0693 - val_loss: 41.9811\n",
      "Epoch 171/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6417 - val_loss: 22.2764\n",
      "Epoch 172/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8729 - val_loss: 1.8085\n",
      "Epoch 173/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7589 - val_loss: 27.5218\n",
      "Epoch 174/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4115 - val_loss: 63.0602\n",
      "Epoch 175/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9923 - val_loss: 53.1722\n",
      "Epoch 176/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3989 - val_loss: 17.3917\n",
      "Epoch 177/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4310 - val_loss: 20.3235\n",
      "Epoch 178/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1747 - val_loss: 20.8952\n",
      "Epoch 179/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7452 - val_loss: 41.7803\n",
      "Epoch 180/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4600 - val_loss: 6.8248\n",
      "Epoch 181/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7806 - val_loss: 102.9280\n",
      "Epoch 182/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.1432 - val_loss: 36.6596\n",
      "Epoch 183/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.9789 - val_loss: 55.1852\n",
      "Epoch 184/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.6300 - val_loss: 14.9062\n",
      "Epoch 185/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.8192 - val_loss: 18.5620\n",
      "Epoch 186/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7598 - val_loss: 2.1784\n",
      "Epoch 187/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4908 - val_loss: 12.7849\n",
      "Epoch 188/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0457 - val_loss: 56.4073\n",
      "Epoch 189/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7263 - val_loss: 51.8989\n",
      "Epoch 190/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8366 - val_loss: 38.0256\n",
      "Epoch 191/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.4097 - val_loss: 22.5230\n",
      "Epoch 192/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6100 - val_loss: 30.3822\n",
      "Epoch 193/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1435 - val_loss: 47.1727\n",
      "Epoch 194/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3193 - val_loss: 11.3563\n",
      "Epoch 195/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6183 - val_loss: 7.8920\n",
      "Epoch 196/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.2295 - val_loss: 17.6617\n",
      "Epoch 197/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6698 - val_loss: 18.7009\n",
      "Epoch 198/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.5918 - val_loss: 11.9165\n",
      "Epoch 199/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7077 - val_loss: 69.6879\n",
      "Epoch 200/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7553 - val_loss: 52.5144\n",
      "Epoch 201/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6293 - val_loss: 6.9799\n",
      "Epoch 202/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4871 - val_loss: 11.3844\n",
      "Epoch 203/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.9274 - val_loss: 35.0208\n",
      "Epoch 204/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5319 - val_loss: 13.1588\n",
      "Epoch 205/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0415 - val_loss: 61.6170\n",
      "Epoch 206/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4049 - val_loss: 21.6187\n",
      "Epoch 207/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8120 - val_loss: 9.7043\n",
      "Epoch 208/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.3063 - val_loss: 17.4625\n",
      "Epoch 209/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 29.0432 - val_loss: 4.1799\n",
      "Epoch 210/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4753 - val_loss: 43.5574\n",
      "Epoch 211/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4188 - val_loss: 24.2010\n",
      "Epoch 212/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0795 - val_loss: 52.0622\n",
      "Epoch 213/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.8301 - val_loss: 59.1632\n",
      "Epoch 214/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.2400 - val_loss: 13.5654\n",
      "Epoch 215/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0624 - val_loss: 7.6144\n",
      "Epoch 216/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.5711 - val_loss: 15.4501\n",
      "Epoch 217/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6014 - val_loss: 23.6219\n",
      "Epoch 218/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.1705 - val_loss: 14.0582\n",
      "Epoch 219/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.2183 - val_loss: 15.7330\n",
      "Epoch 220/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9237 - val_loss: 28.3482\n",
      "Epoch 221/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4905 - val_loss: 27.5680\n",
      "Epoch 222/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0388 - val_loss: 149.7565\n",
      "Epoch 223/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1450 - val_loss: 14.0288\n",
      "Epoch 224/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0206 - val_loss: 33.2093\n",
      "Epoch 225/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8675 - val_loss: 15.1374\n",
      "Epoch 226/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.1842 - val_loss: 23.9923\n",
      "Epoch 227/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6188 - val_loss: 34.2532\n",
      "Epoch 228/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6411 - val_loss: 2.0034\n",
      "Epoch 229/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5766 - val_loss: 63.8682\n",
      "Epoch 230/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.5934 - val_loss: 44.6996\n",
      "Epoch 231/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.0690 - val_loss: 47.7382\n",
      "Epoch 232/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.5408 - val_loss: 77.2433\n",
      "Epoch 233/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5229 - val_loss: 4.6414\n",
      "Epoch 234/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.0108 - val_loss: 83.6742\n",
      "Epoch 235/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.5734 - val_loss: 35.3735\n",
      "Epoch 236/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3070 - val_loss: 14.1427\n",
      "Epoch 237/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.2842 - val_loss: 16.5516\n",
      "Epoch 238/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.3652 - val_loss: 55.5455\n",
      "Epoch 239/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.3160 - val_loss: 15.8394\n",
      "Epoch 240/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.8732 - val_loss: 148.2550\n",
      "Epoch 241/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7673 - val_loss: 17.2378\n",
      "Epoch 242/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9078 - val_loss: 45.3386\n",
      "Epoch 243/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4511 - val_loss: 2.1143\n",
      "Epoch 244/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3000 - val_loss: 28.0445\n",
      "Epoch 245/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.4129 - val_loss: 72.4215\n",
      "Epoch 246/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.3417 - val_loss: 103.4582\n",
      "Epoch 247/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.6444 - val_loss: 35.2580\n",
      "Epoch 248/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.2941 - val_loss: 19.3365\n",
      "Epoch 249/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1988 - val_loss: 25.0226\n",
      "Epoch 250/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8347 - val_loss: 63.8137\n",
      "Epoch 251/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 30.0441 - val_loss: 12.5455\n",
      "Epoch 252/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6329 - val_loss: 5.0625\n",
      "Epoch 253/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 36.6598 - val_loss: 17.1860\n",
      "Epoch 254/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 11.6525 - val_loss: 83.2909\n",
      "Epoch 255/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.4415 - val_loss: 8.7457\n",
      "Epoch 256/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7216 - val_loss: 1.3803\n",
      "Epoch 257/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.5484 - val_loss: 12.8085\n",
      "Epoch 258/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5857 - val_loss: 43.3444\n",
      "Epoch 259/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5207 - val_loss: 25.4232\n",
      "Epoch 260/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1408 - val_loss: 9.4463\n",
      "Epoch 261/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.5774 - val_loss: 9.9480\n",
      "Epoch 262/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.3811 - val_loss: 75.1708\n",
      "Epoch 263/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.1952 - val_loss: 25.5474\n",
      "Epoch 264/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4963 - val_loss: 19.1219\n",
      "Epoch 265/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.3043 - val_loss: 7.6583\n",
      "Epoch 266/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9613 - val_loss: 6.8789\n",
      "Epoch 267/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.8854 - val_loss: 6.7486\n",
      "Epoch 268/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3144 - val_loss: 36.5547\n",
      "Epoch 269/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.2176 - val_loss: 4.7564\n",
      "Epoch 270/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.1541 - val_loss: 38.4823\n",
      "Epoch 271/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.9738 - val_loss: 4.8822\n",
      "Epoch 272/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.5606 - val_loss: 4.4683\n",
      "Epoch 273/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.1783 - val_loss: 42.7962\n",
      "Epoch 274/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.0183 - val_loss: 22.5642\n",
      "Epoch 275/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.0156 - val_loss: 19.0192\n",
      "Epoch 276/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.8547 - val_loss: 18.8190\n",
      "Epoch 277/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.9659 - val_loss: 16.9250\n",
      "Epoch 278/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.1263 - val_loss: 40.7442\n",
      "Epoch 279/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.4439 - val_loss: 22.7406\n",
      "Epoch 280/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.3102 - val_loss: 8.7953\n",
      "Epoch 281/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.1148 - val_loss: 8.4512\n",
      "Epoch 282/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8746 - val_loss: 1.7895\n",
      "Epoch 283/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6828 - val_loss: 35.0712\n",
      "Epoch 284/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8915 - val_loss: 64.3444\n",
      "Epoch 285/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1063 - val_loss: 32.5634\n",
      "Epoch 286/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 24.8416 - val_loss: 13.3680\n",
      "Epoch 287/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4414 - val_loss: 22.7802\n",
      "Epoch 288/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7765 - val_loss: 54.7097\n",
      "Epoch 289/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1469 - val_loss: 46.5374\n",
      "Epoch 290/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.4439 - val_loss: 37.4455\n",
      "Epoch 291/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8932 - val_loss: 7.2524\n",
      "Epoch 292/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7227 - val_loss: 24.6704\n",
      "Epoch 293/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4405 - val_loss: 37.2626\n",
      "Epoch 294/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8066 - val_loss: 36.0357\n",
      "Epoch 295/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1667 - val_loss: 18.4573\n",
      "Epoch 296/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.6500 - val_loss: 15.4702\n",
      "Epoch 297/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6011 - val_loss: 0.5831\n",
      "Epoch 298/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.9575 - val_loss: 14.4308\n",
      "Epoch 299/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1252 - val_loss: 27.2825\n",
      "Epoch 300/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.8978 - val_loss: 23.6113\n",
      "Epoch 301/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8490 - val_loss: 22.2767\n",
      "Epoch 302/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0903 - val_loss: 7.3355\n",
      "Epoch 303/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7374 - val_loss: 35.5768\n",
      "Epoch 304/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5665 - val_loss: 10.9347\n",
      "Epoch 305/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5036 - val_loss: 5.0725\n",
      "Epoch 306/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.4385 - val_loss: 71.0187\n",
      "Epoch 307/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0421 - val_loss: 31.5346\n",
      "Epoch 308/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.4192 - val_loss: 6.2335\n",
      "Epoch 309/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0963 - val_loss: 10.5026\n",
      "Epoch 310/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.8033 - val_loss: 7.4225\n",
      "Epoch 311/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0344 - val_loss: 36.2526\n",
      "Epoch 312/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8565 - val_loss: 55.7395\n",
      "Epoch 313/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0885 - val_loss: 18.4651\n",
      "Epoch 314/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.3840 - val_loss: 7.1937\n",
      "Epoch 315/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4146 - val_loss: 23.6939\n",
      "Epoch 316/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.8311 - val_loss: 2.7656\n",
      "Epoch 317/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5148 - val_loss: 3.9390\n",
      "Epoch 318/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8389 - val_loss: 4.7847\n",
      "Epoch 319/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3075 - val_loss: 23.9996\n",
      "Epoch 320/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.9559 - val_loss: 13.6901\n",
      "Epoch 321/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9552 - val_loss: 48.2816\n",
      "Epoch 322/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.5135 - val_loss: 6.8538\n",
      "Epoch 323/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 19.0006 - val_loss: 6.6404\n",
      "Epoch 324/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 23.1898 - val_loss: 2.5163\n",
      "Epoch 325/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9526 - val_loss: 6.9150\n",
      "Epoch 326/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.1523 - val_loss: 122.4622\n",
      "Epoch 327/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.4393 - val_loss: 17.9539\n",
      "Epoch 328/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.0912 - val_loss: 25.1447\n",
      "Epoch 329/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6066 - val_loss: 25.0057\n",
      "Epoch 330/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.9839 - val_loss: 22.8487\n",
      "Epoch 331/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9667 - val_loss: 14.4272\n",
      "Epoch 332/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2666 - val_loss: 10.2461\n",
      "Epoch 333/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9357 - val_loss: 63.3489\n",
      "Epoch 334/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.4962 - val_loss: 9.6036\n",
      "Epoch 335/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.2975 - val_loss: 22.4491\n",
      "Epoch 336/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 25.1490 - val_loss: 6.5844\n",
      "Epoch 337/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0294 - val_loss: 0.3736\n",
      "Epoch 338/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6001 - val_loss: 6.5468\n",
      "Epoch 339/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5238 - val_loss: 9.3683\n",
      "Epoch 340/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.4972 - val_loss: 21.8023\n",
      "Epoch 341/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.8661 - val_loss: 23.6966\n",
      "Epoch 342/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 33.0122 - val_loss: 1.6413\n",
      "Epoch 343/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 22.3578 - val_loss: 8.5724\n",
      "Epoch 344/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9805 - val_loss: 16.4502\n",
      "Epoch 345/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3013 - val_loss: 1.2796\n",
      "Epoch 346/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.0802 - val_loss: 21.1211\n",
      "Epoch 347/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.0978 - val_loss: 32.1303\n",
      "Epoch 348/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1657 - val_loss: 17.0489\n",
      "Epoch 349/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5412 - val_loss: 19.7667\n",
      "Epoch 350/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.8523 - val_loss: 62.6803\n",
      "Epoch 351/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.4601 - val_loss: 47.0365\n",
      "Epoch 352/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.5532 - val_loss: 45.2203\n",
      "Epoch 353/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.2034 - val_loss: 50.0558\n",
      "Epoch 354/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.8820 - val_loss: 32.1029\n",
      "Epoch 355/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1848 - val_loss: 11.1446\n",
      "Epoch 356/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.4083 - val_loss: 23.7744\n",
      "Epoch 357/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6751 - val_loss: 27.9405\n",
      "Epoch 358/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.2868 - val_loss: 25.4770\n",
      "Epoch 359/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9561 - val_loss: 2.5713\n",
      "Epoch 360/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4501 - val_loss: 37.6736\n",
      "Epoch 361/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7939 - val_loss: 18.6916\n",
      "Epoch 362/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9623 - val_loss: 13.4940\n",
      "Epoch 363/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7139 - val_loss: 8.8747\n",
      "Epoch 364/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5668 - val_loss: 93.8106\n",
      "Epoch 365/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1837 - val_loss: 53.1124\n",
      "Epoch 366/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0961 - val_loss: 62.5136\n",
      "Epoch 367/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9898 - val_loss: 24.5990\n",
      "Epoch 368/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9701 - val_loss: 22.2669\n",
      "Epoch 369/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2857 - val_loss: 5.8558\n",
      "Epoch 370/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.5081 - val_loss: 19.7171\n",
      "Epoch 371/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3551 - val_loss: 13.4192\n",
      "Epoch 372/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.4097 - val_loss: 21.0010\n",
      "Epoch 373/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6581 - val_loss: 12.4982\n",
      "Epoch 374/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9764 - val_loss: 59.6035\n",
      "Epoch 375/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1665 - val_loss: 20.2001\n",
      "Epoch 376/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.1571 - val_loss: 16.8624\n",
      "Epoch 377/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4238 - val_loss: 39.9994\n",
      "Epoch 378/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2951 - val_loss: 39.2237\n",
      "Epoch 379/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.4194 - val_loss: 22.4850\n",
      "Epoch 380/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5943 - val_loss: 8.7442\n",
      "Epoch 381/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1605 - val_loss: 1.8385\n",
      "Epoch 382/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.7414 - val_loss: 7.6889\n",
      "Epoch 383/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 21.3330 - val_loss: 38.3272\n",
      "Epoch 384/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2722 - val_loss: 1.4472\n",
      "Epoch 385/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4977 - val_loss: 8.5029\n",
      "Epoch 386/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.1224 - val_loss: 16.1373\n",
      "Epoch 387/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.4972 - val_loss: 24.4370\n",
      "Epoch 388/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.2631 - val_loss: 5.1497\n",
      "Epoch 389/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2707 - val_loss: 4.7194\n",
      "Epoch 390/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6589 - val_loss: 24.0876\n",
      "Epoch 391/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.3545 - val_loss: 19.0819\n",
      "Epoch 392/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.6243 - val_loss: 8.1493\n",
      "Epoch 393/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2539 - val_loss: 33.9051\n",
      "Epoch 394/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.2563 - val_loss: 32.4738\n",
      "Epoch 395/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.7778 - val_loss: 41.8659\n",
      "Epoch 396/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.6679 - val_loss: 6.6221\n",
      "Epoch 397/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.9804 - val_loss: 2.2239\n",
      "Epoch 398/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.0616 - val_loss: 34.8852\n",
      "Epoch 399/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2082 - val_loss: 21.8868\n",
      "Epoch 400/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.3245 - val_loss: 15.6601\n",
      "Epoch 401/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6773 - val_loss: 2.0118\n",
      "Epoch 402/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2358 - val_loss: 7.6256\n",
      "Epoch 403/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.1899 - val_loss: 4.3815\n",
      "Epoch 404/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8581 - val_loss: 42.6102\n",
      "Epoch 405/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7854 - val_loss: 4.0553\n",
      "Epoch 406/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3630 - val_loss: 13.1819\n",
      "Epoch 407/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.4350 - val_loss: 0.8760\n",
      "Epoch 408/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.1551 - val_loss: 5.6518\n",
      "Epoch 409/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2990 - val_loss: 4.1176\n",
      "Epoch 410/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3165 - val_loss: 62.4876\n",
      "Epoch 411/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8632 - val_loss: 26.7789\n",
      "Epoch 412/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7182 - val_loss: 37.0745\n",
      "Epoch 413/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8916 - val_loss: 5.6069\n",
      "Epoch 414/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2661 - val_loss: 36.6814\n",
      "Epoch 415/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1992 - val_loss: 20.0058\n",
      "Epoch 416/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9022 - val_loss: 3.7667\n",
      "Epoch 417/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7439 - val_loss: 10.0918\n",
      "Epoch 418/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.6257 - val_loss: 30.7181\n",
      "Epoch 419/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6650 - val_loss: 45.8813\n",
      "Epoch 420/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8410 - val_loss: 60.5321\n",
      "Epoch 421/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.9655 - val_loss: 33.6206\n",
      "Epoch 422/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6736 - val_loss: 9.6917\n",
      "Epoch 423/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2201 - val_loss: 12.6378\n",
      "Epoch 424/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0533 - val_loss: 14.4532\n",
      "Epoch 425/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.2281 - val_loss: 3.0384\n",
      "Epoch 426/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.2027 - val_loss: 9.4014\n",
      "Epoch 427/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.9777 - val_loss: 40.2823\n",
      "Epoch 428/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.2765 - val_loss: 15.7408\n",
      "Epoch 429/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9049 - val_loss: 12.1426\n",
      "Epoch 430/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8063 - val_loss: 29.8434\n",
      "Epoch 431/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.6988 - val_loss: 16.7874\n",
      "Epoch 432/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7802 - val_loss: 32.0451\n",
      "Epoch 433/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.4768 - val_loss: 11.8997\n",
      "Epoch 434/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 17.0845 - val_loss: 3.1165\n",
      "Epoch 435/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.5565 - val_loss: 17.0372\n",
      "Epoch 436/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.3119 - val_loss: 21.8769\n",
      "Epoch 437/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.1485 - val_loss: 9.7990\n",
      "Epoch 438/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5108 - val_loss: 2.6940\n",
      "Epoch 439/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.2947 - val_loss: 10.9826\n",
      "Epoch 440/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.7122 - val_loss: 7.2286\n",
      "Epoch 441/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.1400 - val_loss: 13.1948\n",
      "Epoch 442/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9889 - val_loss: 43.7240\n",
      "Epoch 443/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9756 - val_loss: 17.9132\n",
      "Epoch 444/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0418 - val_loss: 4.4147\n",
      "Epoch 445/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6429 - val_loss: 12.2348\n",
      "Epoch 446/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8889 - val_loss: 10.2455\n",
      "Epoch 447/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9268 - val_loss: 6.4022\n",
      "Epoch 448/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0260 - val_loss: 6.9329\n",
      "Epoch 449/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.3576 - val_loss: 17.9263\n",
      "Epoch 450/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6240 - val_loss: 5.4099\n",
      "Epoch 451/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.1513 - val_loss: 8.8161\n",
      "Epoch 452/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.1582 - val_loss: 8.1906\n",
      "Epoch 453/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.7201 - val_loss: 9.8348\n",
      "Epoch 454/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.5777 - val_loss: 20.2324\n",
      "Epoch 455/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.9396 - val_loss: 11.3284\n",
      "Epoch 456/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1494 - val_loss: 11.3949\n",
      "Epoch 457/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.5848 - val_loss: 13.1424\n",
      "Epoch 458/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9534 - val_loss: 12.1289\n",
      "Epoch 459/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9568 - val_loss: 12.1412\n",
      "Epoch 460/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.3401 - val_loss: 13.1974\n",
      "Epoch 461/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.2064 - val_loss: 3.8829\n",
      "Epoch 462/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7471 - val_loss: 19.5772\n",
      "Epoch 463/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4407 - val_loss: 13.5525\n",
      "Epoch 464/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9016 - val_loss: 17.2254\n",
      "Epoch 465/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.4333 - val_loss: 8.5062\n",
      "Epoch 466/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.4108 - val_loss: 3.4028\n",
      "Epoch 467/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7193 - val_loss: 21.7777\n",
      "Epoch 468/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.2791 - val_loss: 7.8753\n",
      "Epoch 469/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8140 - val_loss: 20.5555\n",
      "Epoch 470/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7967 - val_loss: 5.4012\n",
      "Epoch 471/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.4845 - val_loss: 0.1469\n",
      "Epoch 472/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.5724 - val_loss: 5.9491\n",
      "Epoch 473/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9730 - val_loss: 16.5674\n",
      "Epoch 474/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0232 - val_loss: 9.5821\n",
      "Epoch 475/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.2120 - val_loss: 33.4141\n",
      "Epoch 476/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.3588 - val_loss: 2.6746\n",
      "Epoch 477/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5176 - val_loss: 20.9527\n",
      "Epoch 478/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.5628 - val_loss: 14.7233\n",
      "Epoch 479/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.7766 - val_loss: 7.8172\n",
      "Epoch 480/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.1522 - val_loss: 4.2428\n",
      "Epoch 481/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6841 - val_loss: 4.5570\n",
      "Epoch 482/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9793 - val_loss: 35.1087\n",
      "Epoch 483/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5937 - val_loss: 36.1140\n",
      "Epoch 484/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.8383 - val_loss: 7.1671\n",
      "Epoch 485/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2199 - val_loss: 5.2755\n",
      "Epoch 486/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.9175 - val_loss: 8.2394\n",
      "Epoch 487/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.1598 - val_loss: 17.3445\n",
      "Epoch 488/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.2929 - val_loss: 6.8002\n",
      "Epoch 489/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 19.8102 - val_loss: 10.8213\n",
      "Epoch 490/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.2807 - val_loss: 18.2355\n",
      "Epoch 491/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.7424 - val_loss: 47.4517\n",
      "Epoch 492/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2065 - val_loss: 14.1415\n",
      "Epoch 493/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.3389 - val_loss: 11.1524\n",
      "Epoch 494/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.9956 - val_loss: 8.1552\n",
      "Epoch 495/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.4934 - val_loss: 27.1275\n",
      "Epoch 496/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.1383 - val_loss: 10.8951\n",
      "Epoch 497/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.4165 - val_loss: 1.0751\n",
      "Epoch 498/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.5119 - val_loss: 8.3591\n",
      "Epoch 499/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7002 - val_loss: 34.8730\n",
      "Epoch 500/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 10.4739 - val_loss: 4.9558\n",
      "Epoch 501/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7774 - val_loss: 17.8234\n",
      "Epoch 502/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.6236 - val_loss: 8.6703\n",
      "Epoch 503/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.0531 - val_loss: 4.6268\n",
      "Epoch 504/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2164 - val_loss: 3.1239\n",
      "Epoch 505/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.5280 - val_loss: 5.5734\n",
      "Epoch 506/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0340 - val_loss: 4.8169\n",
      "Epoch 507/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5307 - val_loss: 16.2418\n",
      "Epoch 508/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.3779 - val_loss: 12.4868\n",
      "Epoch 509/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.2847 - val_loss: 21.3694\n",
      "Epoch 510/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.2271 - val_loss: 2.4326\n",
      "Epoch 511/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5831 - val_loss: 14.1560\n",
      "Epoch 512/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.0343 - val_loss: 13.8778\n",
      "Epoch 513/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.9669 - val_loss: 11.8397\n",
      "Epoch 514/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0845 - val_loss: 14.6407\n",
      "Epoch 515/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.9668 - val_loss: 24.2941\n",
      "Epoch 516/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4555 - val_loss: 6.0594\n",
      "Epoch 517/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6154 - val_loss: 9.7941\n",
      "Epoch 518/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.9758 - val_loss: 6.3668\n",
      "Epoch 519/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8944 - val_loss: 8.9280\n",
      "Epoch 520/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5354 - val_loss: 35.8911\n",
      "Epoch 521/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3822 - val_loss: 8.5179\n",
      "Epoch 522/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9563 - val_loss: 1.9591\n",
      "Epoch 523/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5658 - val_loss: 47.1562\n",
      "Epoch 524/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6278 - val_loss: 49.4812\n",
      "Epoch 525/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.5188 - val_loss: 8.5875\n",
      "Epoch 526/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.2097 - val_loss: 5.8913\n",
      "Epoch 527/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7010 - val_loss: 17.2133\n",
      "Epoch 528/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.6925 - val_loss: 31.0422\n",
      "Epoch 529/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.9240 - val_loss: 22.1851\n",
      "Epoch 530/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.9248 - val_loss: 41.4215\n",
      "Epoch 531/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8451 - val_loss: 3.6509\n",
      "Epoch 532/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7717 - val_loss: 29.5880\n",
      "Epoch 533/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4868 - val_loss: 1.6002\n",
      "Epoch 534/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.2274 - val_loss: 16.4785\n",
      "Epoch 535/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.5459 - val_loss: 12.2383\n",
      "Epoch 536/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.1201 - val_loss: 22.3662\n",
      "Epoch 537/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7811 - val_loss: 17.7220\n",
      "Epoch 538/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.4630 - val_loss: 9.9050\n",
      "Epoch 539/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 2.5765 - val_loss: 1.7023\n",
      "Epoch 540/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.6025 - val_loss: 1.8828\n",
      "Epoch 541/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8750 - val_loss: 42.1613\n",
      "Epoch 542/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8004 - val_loss: 20.3410\n",
      "Epoch 543/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5195 - val_loss: 9.3001\n",
      "Epoch 544/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 7.9438 - val_loss: 10.3465\n",
      "Epoch 545/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.8399 - val_loss: 11.8082\n",
      "Epoch 546/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.9173 - val_loss: 1.6116\n",
      "Epoch 547/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6726 - val_loss: 26.6406\n",
      "Epoch 548/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3489 - val_loss: 24.1835\n",
      "Epoch 549/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6036 - val_loss: 6.8641\n",
      "Epoch 550/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6894 - val_loss: 1.9131\n",
      "Epoch 551/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.1685 - val_loss: 8.4501\n",
      "Epoch 552/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5079 - val_loss: 3.3509\n",
      "Epoch 553/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5797 - val_loss: 1.5436\n",
      "Epoch 554/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.0345 - val_loss: 16.6781\n",
      "Epoch 555/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8255 - val_loss: 8.3078\n",
      "Epoch 556/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.7919 - val_loss: 35.1578\n",
      "Epoch 557/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3005 - val_loss: 11.9690\n",
      "Epoch 558/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9044 - val_loss: 2.5454\n",
      "Epoch 559/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.6383 - val_loss: 13.8967\n",
      "Epoch 560/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0381 - val_loss: 3.0227\n",
      "Epoch 561/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8258 - val_loss: 9.3111\n",
      "Epoch 562/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.3767 - val_loss: 1.4200\n",
      "Epoch 563/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.3046 - val_loss: 13.4672\n",
      "Epoch 564/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6858 - val_loss: 2.6166\n",
      "Epoch 565/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6317 - val_loss: 23.4813\n",
      "Epoch 566/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 11.5124 - val_loss: 16.8706\n",
      "Epoch 567/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.9318 - val_loss: 29.8004\n",
      "Epoch 568/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.0369 - val_loss: 11.4652\n",
      "Epoch 569/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9409 - val_loss: 15.6200\n",
      "Epoch 570/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.7414 - val_loss: 17.0117\n",
      "Epoch 571/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9268 - val_loss: 14.9435\n",
      "Epoch 572/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0871 - val_loss: 21.9049\n",
      "Epoch 573/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4928 - val_loss: 17.6715\n",
      "Epoch 574/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.2034 - val_loss: 9.0643\n",
      "Epoch 575/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6200 - val_loss: 4.9092\n",
      "Epoch 576/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0806 - val_loss: 11.9298\n",
      "Epoch 577/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.3448 - val_loss: 3.1611\n",
      "Epoch 578/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5230 - val_loss: 4.5033\n",
      "Epoch 579/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9895 - val_loss: 6.5158\n",
      "Epoch 580/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.3894 - val_loss: 1.4253\n",
      "Epoch 581/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0756 - val_loss: 15.5759\n",
      "Epoch 582/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 15.4362 - val_loss: 19.2361\n",
      "Epoch 583/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3298 - val_loss: 15.3550\n",
      "Epoch 584/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 26.0240 - val_loss: 2.5426\n",
      "Epoch 585/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7918 - val_loss: 28.4809\n",
      "Epoch 586/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6621 - val_loss: 9.8923\n",
      "Epoch 587/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5480 - val_loss: 2.6004\n",
      "Epoch 588/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9573 - val_loss: 28.5609\n",
      "Epoch 589/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 16.5916 - val_loss: 14.9014\n",
      "Epoch 590/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3945 - val_loss: 8.5259\n",
      "Epoch 591/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8830 - val_loss: 1.2154\n",
      "Epoch 592/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.9655 - val_loss: 2.2634\n",
      "Epoch 593/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8662 - val_loss: 8.8416\n",
      "Epoch 594/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.1074 - val_loss: 13.0751\n",
      "Epoch 595/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.5900 - val_loss: 14.4130\n",
      "Epoch 596/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 16.7439 - val_loss: 15.5282\n",
      "Epoch 597/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8018 - val_loss: 22.3818\n",
      "Epoch 598/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 17.8085 - val_loss: 4.6118\n",
      "Epoch 599/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7119 - val_loss: 6.3168\n",
      "Epoch 600/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0401 - val_loss: 20.8791\n",
      "Epoch 601/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6707 - val_loss: 9.5379\n",
      "Epoch 602/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.6893 - val_loss: 17.1410\n",
      "Epoch 603/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9365 - val_loss: 22.4026\n",
      "Epoch 604/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0593 - val_loss: 19.3979\n",
      "Epoch 605/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9177 - val_loss: 16.8463\n",
      "Epoch 606/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.5113 - val_loss: 6.2677\n",
      "Epoch 607/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8796 - val_loss: 6.5096\n",
      "Epoch 608/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7460 - val_loss: 9.0642\n",
      "Epoch 609/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.2907 - val_loss: 3.1935\n",
      "Epoch 610/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.1062 - val_loss: 33.0396\n",
      "Epoch 611/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6504 - val_loss: 2.3436\n",
      "Epoch 612/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.4491 - val_loss: 7.8256\n",
      "Epoch 613/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.2765 - val_loss: 11.6874\n",
      "Epoch 614/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0235 - val_loss: 20.6699\n",
      "Epoch 615/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 8.0321 - val_loss: 3.5989\n",
      "Epoch 616/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.5054 - val_loss: 6.3151\n",
      "Epoch 617/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.6217 - val_loss: 0.7152\n",
      "Epoch 618/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1481 - val_loss: 3.1114\n",
      "Epoch 619/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3082 - val_loss: 2.0246\n",
      "Epoch 620/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2609 - val_loss: 6.6691\n",
      "Epoch 621/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4984 - val_loss: 4.1284\n",
      "Epoch 622/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3287 - val_loss: 28.7170\n",
      "Epoch 623/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0974 - val_loss: 1.2115\n",
      "Epoch 624/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 9.1272 - val_loss: 13.2041\n",
      "Epoch 625/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.0446 - val_loss: 15.9618\n",
      "Epoch 626/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.5478 - val_loss: 36.4798\n",
      "Epoch 627/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4985 - val_loss: 4.7908\n",
      "Epoch 628/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 21.7079 - val_loss: 28.8659\n",
      "Epoch 629/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8028 - val_loss: 6.4332\n",
      "Epoch 630/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.0263 - val_loss: 12.8399\n",
      "Epoch 631/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4194 - val_loss: 1.7686\n",
      "Epoch 632/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 13.3229 - val_loss: 1.4438\n",
      "Epoch 633/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.0019 - val_loss: 6.3075\n",
      "Epoch 634/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2761 - val_loss: 18.3570\n",
      "Epoch 635/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4701 - val_loss: 17.8045\n",
      "Epoch 636/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.5749 - val_loss: 19.2650\n",
      "Epoch 637/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.4738 - val_loss: 4.9024\n",
      "Epoch 638/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.7470 - val_loss: 4.3577\n",
      "Epoch 639/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.7000 - val_loss: 5.9819\n",
      "Epoch 640/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.4689 - val_loss: 22.8739\n",
      "Epoch 641/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2767 - val_loss: 2.8357\n",
      "Epoch 642/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.1461 - val_loss: 0.9088\n",
      "Epoch 643/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 12.5315 - val_loss: 1.3770\n",
      "Epoch 644/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.1851 - val_loss: 2.8672\n",
      "Epoch 645/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.8133 - val_loss: 17.0423\n",
      "Epoch 646/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 18.6941 - val_loss: 2.6932\n",
      "Epoch 647/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 16.7430 - val_loss: 27.9573\n",
      "Epoch 648/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 20.4779 - val_loss: 46.7804\n",
      "Epoch 649/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2735 - val_loss: 3.8086\n",
      "Epoch 650/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.9593 - val_loss: 13.2904\n",
      "Epoch 651/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.0667 - val_loss: 5.0079\n",
      "Epoch 652/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.8621 - val_loss: 7.9360\n",
      "Epoch 653/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.1714 - val_loss: 4.7017\n",
      "Epoch 654/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.0306 - val_loss: 10.0955\n",
      "Epoch 655/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.0145 - val_loss: 3.0207\n",
      "Epoch 656/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 13.0706 - val_loss: 6.4739\n",
      "Epoch 657/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.5329 - val_loss: 14.4659\n",
      "Epoch 658/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.2409 - val_loss: 1.5180\n",
      "Epoch 659/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.7510 - val_loss: 8.0600\n",
      "Epoch 660/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3787 - val_loss: 8.6084\n",
      "Epoch 661/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.4301 - val_loss: 0.6331\n",
      "Epoch 662/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.7009 - val_loss: 4.7524\n",
      "Epoch 663/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.1099 - val_loss: 26.1248\n",
      "Epoch 664/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6588 - val_loss: 8.8154\n",
      "Epoch 665/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.6818 - val_loss: 9.6150\n",
      "Epoch 666/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.3390 - val_loss: 4.3848\n",
      "Epoch 667/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.2017 - val_loss: 5.2730\n",
      "Epoch 668/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9700 - val_loss: 5.9744\n",
      "Epoch 669/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.6170 - val_loss: 12.7808\n",
      "Epoch 670/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.8517 - val_loss: 1.8154\n",
      "Epoch 671/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.3328 - val_loss: 11.9744\n",
      "Epoch 672/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 12.0435 - val_loss: 31.8857\n",
      "Epoch 673/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4621 - val_loss: 10.8140\n",
      "Epoch 674/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 5.9489 - val_loss: 8.3607\n",
      "Epoch 675/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0013 - val_loss: 10.7771\n",
      "Epoch 676/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.3876 - val_loss: 5.2291\n",
      "Epoch 677/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.8446 - val_loss: 16.2074\n",
      "Epoch 678/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.6669 - val_loss: 10.1104\n",
      "Epoch 679/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.5275 - val_loss: 5.7731\n",
      "Epoch 680/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.8474 - val_loss: 3.7986\n",
      "Epoch 681/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.7324 - val_loss: 14.8259\n",
      "Epoch 682/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.0104 - val_loss: 10.5038\n",
      "Epoch 683/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.5453 - val_loss: 4.5186\n",
      "Epoch 684/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9141 - val_loss: 10.2058\n",
      "Epoch 685/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 10.4458 - val_loss: 17.6946\n",
      "Epoch 686/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.8789 - val_loss: 3.0795\n",
      "Epoch 687/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 8.7070 - val_loss: 11.8933\n",
      "Epoch 688/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.7656 - val_loss: 29.3626\n",
      "Epoch 689/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 14.0539 - val_loss: 31.1786\n",
      "Epoch 690/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 5.9519 - val_loss: 8.3822\n",
      "Epoch 691/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.2771 - val_loss: 6.3116\n",
      "Epoch 692/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3145 - val_loss: 19.0742\n",
      "Epoch 693/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.7681 - val_loss: 12.9211\n",
      "Epoch 694/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7207 - val_loss: 6.1747\n",
      "Epoch 695/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3170 - val_loss: 14.9216\n",
      "Epoch 696/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 6.0350 - val_loss: 2.9764\n",
      "Epoch 697/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 9.3175 - val_loss: 13.6117\n",
      "Epoch 698/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 7.4622 - val_loss: 5.3464\n",
      "Epoch 699/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 11.8443 - val_loss: 8.1314\n",
      "Epoch 700/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1584 - val_loss: 5.1288\n",
      "Epoch 701/1000\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.9813 - val_loss: 4.4691\n",
      "Epoch 702/1000\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 6.0790 - val_loss: 5.1900\n",
      "Epoch 703/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.8939"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer=optimizer) \n",
    "# model2_train = model.fit(zz_train, yy_train, \n",
    "# #                          validation_split=0.0, \n",
    "#                          batch_size=batch_size, epochs=1000, verbose=1,\n",
    "#                          validation_data=(zz_test, yy_test))    \n",
    "model_train = model.fit(z_train, y_train, batch_size=batch_size,\n",
    "                                    epochs=1000, verbose=1,\n",
    "                                    validation_data=(z_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train.history['val_loss'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data\n",
    "test_batch_size = 500\n",
    "# x_test = np.random.uniform(0, 2., test_batch_size)\n",
    "x_test = np.linspace(0, 1.5, test_batch_size)\n",
    "\n",
    "mc_dropout_num = 100  # Run Dropout 100 times\n",
    "predictions = np.zeros((mc_dropout_num, test_batch_size, 2))\n",
    "uncertainty = np.zeros((mc_dropout_num, test_batch_size, 1))\n",
    "for i in range(mc_dropout_num):\n",
    "    predictions[i] = model.predict(scalerz.transform(x_test.reshape(-1,1)))\n",
    "    \n",
    "# get mean results and its varience\n",
    "prediction_mc_dropout = np.mean(predictions, axis=0)\n",
    "std_mc_dropout = np.std(predictions, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 7), dpi=100)\n",
    "# plt.figure(figsize=(5, 4), dpi=100)\n",
    "sigma = np.sqrt(std_mc_dropout[:, 0]**2 + std_mc_dropout[:, 1]**2+ prediction_mc_dropout[:,1]**2)\n",
    "\n",
    "\n",
    "# plt.scatter(scalerz.inverse_transform(zz), yy, color='k', label='Original cosmic chronometers')\n",
    "plt.plot(zmodel, flcdm, label='$\\Lambda CDM$', c='b')\n",
    "plt.errorbar(dfp['zCMB'], dfp['MU']-19, dfp['errors'], fmt='g.', markersize=10, label='Observations')\n",
    "plt.errorbar(x_test, prediction_mc_dropout[:,0]-19, yerr=sigma, markersize=2, fmt='o', \n",
    "             ecolor='r', capthick=2, elinewidth=0.8, alpha=0.3, c='m',\n",
    "             label='Synthetic data with $\\sigma$')\n",
    "\n",
    "plt.xlim(0,1.5)\n",
    "\n",
    "# plt.xlabel('Redshift')\n",
    "# plt.ylabel('$H(z)$')\n",
    "plt.ylabel(\"$\\mu(z)$\", fontsize=20)\n",
    "plt.xlabel(\"Redshift z\", fontsize=20)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "# plt.legend(loc='best')\n",
    "plt.legend(loc='upper left')\n",
    "# plt.show()\n",
    "plt.tight_layout()\n",
    "\n",
    "# zCMB \tMU \terrors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6), dpi=100)\n",
    "\n",
    "plt.plot(model_train.history['loss'], color='r', )\n",
    "plt.plot(model_train.history['val_loss'], color='g')\n",
    "\n",
    "plt.ylabel('MSE', fontsize=11)\n",
    "plt.xlabel('Epoch', fontsize=11)\n",
    "plt.legend(['training set', 'validation set'], loc='upper right', fontsize=12)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.ylim(0, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
